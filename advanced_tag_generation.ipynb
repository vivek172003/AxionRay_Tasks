{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28301911",
   "metadata": {},
   "source": [
    "# Natural Language Processing with NLTK for Tag Generation\n",
    "\n",
    "In this notebook, we'll explore how to use the Natural Language Toolkit (NLTK) to generate tags from textual data in vehicle repair records. While regular expressions (regex) are useful for simple pattern matching, NLTK provides more powerful text processing capabilities specifically designed for natural language processing.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "1. Introduction to NLTK and why it's used for text processing\n",
    "2. Text preprocessing with NLTK (tokenization, stopword removal, stemming, lemmatization)\n",
    "3. Part-of-speech tagging to identify important terms\n",
    "4. Named entity recognition for identifying specific components\n",
    "5. Frequency analysis and collocation detection\n",
    "6. Generating meaningful tags from repair descriptions\n",
    "\n",
    "This approach will improve upon the basic regex method we used previously while remaining accessible and easy to explain in an interview setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead13a0e",
   "metadata": {},
   "source": [
    "## 1. Introduction to NLTK: What is it and Why Use it?\n",
    "\n",
    "### What is NLTK?\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources (like WordNet), along with a suite of text processing libraries for:\n",
    "\n",
    "- Tokenization (splitting text into words or sentences)\n",
    "- Stemming and lemmatization (reducing words to their base forms)\n",
    "- Part-of-speech tagging (identifying nouns, verbs, adjectives, etc.)\n",
    "- Named entity recognition (identifying names of people, organizations, locations, etc.)\n",
    "- Parsing and semantic reasoning\n",
    "\n",
    "### Why Use NLTK for Tag Generation?\n",
    "\n",
    "1. **Specialized for Language Processing**: Unlike regex, which is a general pattern-matching tool, NLTK is specifically designed for natural language processing tasks.\n",
    "\n",
    "2. **Linguistic Knowledge**: NLTK incorporates linguistic knowledge that helps it understand language structure and meaning, not just character patterns.\n",
    "\n",
    "3. **Pre-built Resources**: It comes with pre-built resources like stopword lists, stemmers, and lexicons that would be tedious to create manually.\n",
    "\n",
    "4. **More Accurate Results**: By understanding language context and structure, NLTK can generate more meaningful and accurate tags than simple regex.\n",
    "\n",
    "5. **Industry Standard**: NLTK is widely used in industry and academia, making it a valuable skill to demonstrate in interviews.\n",
    "\n",
    "Let's start by importing the necessary libraries and preparing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b009db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Import entire NLTK library\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources all at once\n",
    "print(\"Downloading necessary NLTK resources...\")\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"You can ignore download errors if the resources are already available.\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NLTK version:\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349ca05",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Dataset\n",
    "\n",
    "Now, let's load the vehicle repair dataset we previously worked with. For our NLTK-based tag generation, we'll focus on the text columns that contain descriptions of problems and repairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fe9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved cleaned data\n",
    "try:\n",
    "    # First try to load the cleaned data from the previous notebook\n",
    "    df = pd.read_csv('cleaned_and_tagged_data_final1.csv')\n",
    "    print(\"Loaded cleaned data from previous notebook\")\n",
    "except FileNotFoundError:\n",
    "    # If the cleaned data is not available, load and clean the original data\n",
    "    print(\"Cleaned data not found, loading original data\")\n",
    "    df = pd.read_excel('DA -Task 2..xlsx')\n",
    "    \n",
    "    # Basic cleaning (simplified version of the previous notebook)\n",
    "    # Fill missing values in object columns with 'Unknown' and numeric columns with mean\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        elif df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ee54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's focus on the text columns we'll use for tag generation\n",
    "text_columns = ['CUSTOMER_VERBATIM', 'CORRECTION_VERBATIM']\n",
    "\n",
    "# Check if these columns exist in the dataframe\n",
    "text_columns = [col for col in text_columns if col in df.columns]\n",
    "\n",
    "if not text_columns:\n",
    "    raise ValueError(\"Required text columns not found in the dataset\")\n",
    "\n",
    "# Preview the text data we'll be working with\n",
    "print(\"Sample text data from each column:\")\n",
    "for col in text_columns:\n",
    "    print(f\"\\n{col} (first 3 examples):\")\n",
    "    for i, text in enumerate(df[col].head(3)):\n",
    "        print(f\"{i+1}. {text}\")\n",
    "        \n",
    "# Combine the text fields as we did in the basic approach\n",
    "df['combined_text'] = df[text_columns].apply(\n",
    "    lambda x: ' '.join([str(text) for text in x if not pd.isna(text)]), axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nCombined text examples (first 3):\")\n",
    "for i, text in enumerate(df['combined_text'].head(3)):\n",
    "    print(f\"{i+1}. {text[:200]}...\" if len(text) > 200 else f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3151e17",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing with NLTK\n",
    "\n",
    "Text preprocessing is a critical step in any NLP pipeline. NLTK provides several tools to clean and normalize text data before further analysis. Let's understand each preprocessing step:\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "**What is it?** Breaking text into smaller units (tokens) such as words or sentences.\n",
    "\n",
    "**Why use it?** \n",
    "- Text needs to be split into words before it can be analyzed\n",
    "- NLTK's tokenizers understand language rules better than simple splitting by spaces\n",
    "- Handles punctuation and special cases properly\n",
    "\n",
    "**NLTK Functions:**\n",
    "- `word_tokenize()`: Splits text into words\n",
    "- `sent_tokenize()`: Splits text into sentences\n",
    "\n",
    "### 2. Stopword Removal\n",
    "\n",
    "**What is it?** Filtering out common words that add little meaning (e.g., \"the\", \"is\", \"and\").\n",
    "\n",
    "**Why use it?**\n",
    "- Reduces noise in the data\n",
    "- Focuses analysis on meaningful content words\n",
    "- Improves efficiency by reducing the number of tokens to process\n",
    "\n",
    "**NLTK Resources:**\n",
    "- `nltk.corpus.stopwords`: Pre-defined lists of stopwords in multiple languages\n",
    "\n",
    "### 3. Stemming vs. Lemmatization\n",
    "\n",
    "**Stemming:**\n",
    "- Removes word endings to get the stem/root\n",
    "- Fast but often produces non-real words\n",
    "- Example: \"running\" → \"run\", \"easily\" → \"easili\"\n",
    "\n",
    "**Lemmatization:**\n",
    "- Reduces words to their dictionary base form\n",
    "- Slower but produces real words\n",
    "- Example: \"running\" → \"run\", \"better\" → \"good\"\n",
    "\n",
    "**Why use these?**\n",
    "- Normalizes variations of the same word\n",
    "- Reduces vocabulary size\n",
    "- Groups related terms together\n",
    "\n",
    "**NLTK Tools:**\n",
    "- `PorterStemmer()`: Popular stemming algorithm\n",
    "- `WordNetLemmatizer()`: Lemmatization using WordNet dictionary\n",
    "\n",
    "### 4. Part-of-Speech (POS) Tagging\n",
    "\n",
    "**What is it?** Identifying grammatical parts of speech (noun, verb, adjective, etc.)\n",
    "\n",
    "**Why use it?**\n",
    "- Helps understand word function in context\n",
    "- Allows filtering for specific types of words (e.g., nouns for technical terms)\n",
    "- Improves accuracy of lemmatization\n",
    "\n",
    "**NLTK Function:**\n",
    "- `pos_tag()`: Tags words with their part of speech\n",
    "\n",
    "Let's implement these preprocessing steps on our vehicle repair data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe340c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive text preprocessing function using NLTK\n",
    "def preprocess_text(text, tokenize=True, remove_stopwords=True, lemmatize=True, \n",
    "                   stem=False, pos_filter=None):\n",
    "    \"\"\"\n",
    "    Preprocess text using NLTK's tools\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to preprocess\n",
    "        tokenize (bool): Whether to tokenize the text\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        lemmatize (bool): Whether to perform lemmatization\n",
    "        stem (bool): Whether to perform stemming (not used if lemmatize is True)\n",
    "        pos_filter (list): List of POS tags to keep (e.g., ['NN', 'NNS'] for nouns)\n",
    "        \n",
    "    Returns:\n",
    "        list or str: Preprocessed tokens or joined text\n",
    "    \"\"\"\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    if tokenize:\n",
    "        tokens = word_tokenize(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords if specified\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    # Apply POS tagging and filter by specified tags if needed\n",
    "    if pos_filter:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens = [word for word, tag in tagged_tokens if tag in pos_filter]\n",
    "    \n",
    "    # Lemmatize tokens if specified\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Stem tokens if lemmatization is not used\n",
    "    elif stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply basic preprocessing to the combined text\n",
    "print(\"Preprocessing text data with NLTK...\")\n",
    "df['combined_text'] = df[text_columns].apply(\n",
    "    lambda x: ' '.join([str(text) for text in x if not pd.isna(text)]), axis=1\n",
    ")\n",
    "\n",
    "# Process with different NLTK techniques to demonstrate\n",
    "df['tokenized'] = df['combined_text'].apply(\n",
    "    lambda x: preprocess_text(x, tokenize=True, remove_stopwords=True, lemmatize=False, stem=False)\n",
    ")\n",
    "\n",
    "df['lemmatized'] = df['combined_text'].apply(\n",
    "    lambda x: preprocess_text(x, tokenize=True, remove_stopwords=True, lemmatize=True, stem=False)\n",
    ")\n",
    "\n",
    "df['stemmed'] = df['combined_text'].apply(\n",
    "    lambda x: preprocess_text(x, tokenize=True, remove_stopwords=True, lemmatize=False, stem=True)\n",
    ")\n",
    "\n",
    "# Extract nouns (often the most important for technical text)\n",
    "df['nouns_only'] = df['combined_text'].apply(\n",
    "    lambda x: preprocess_text(x, tokenize=True, remove_stopwords=True, \n",
    "                            lemmatize=True, pos_filter=['NN', 'NNS'])\n",
    ")\n",
    "\n",
    "# Display examples to show the difference between techniques\n",
    "print(\"\\nNLTK Preprocessing Examples:\")\n",
    "sample_idx = 0  # Use the first document as an example\n",
    "\n",
    "print(f\"\\nOriginal Text:\")\n",
    "print(df['combined_text'].iloc[sample_idx][:200] + \"...\" if len(df['combined_text'].iloc[sample_idx]) > 200 \n",
    "      else df['combined_text'].iloc[sample_idx])\n",
    "\n",
    "print(f\"\\nAfter Tokenization and Stopword Removal:\")\n",
    "print(df['tokenized'].iloc[sample_idx][:15])\n",
    "\n",
    "print(f\"\\nAfter Lemmatization:\")\n",
    "print(df['lemmatized'].iloc[sample_idx][:15])\n",
    "\n",
    "print(f\"\\nAfter Stemming:\")\n",
    "print(df['stemmed'].iloc[sample_idx][:15])\n",
    "\n",
    "print(f\"\\nNouns Only:\")\n",
    "print(df['nouns_only'].iloc[sample_idx][:15])\n",
    "\n",
    "# Create a function to compare the techniques\n",
    "def compare_preprocessing(text):\n",
    "    \"\"\"Compare different preprocessing techniques on a sample text\"\"\"\n",
    "    original = text\n",
    "    tokenized = preprocess_text(text, lemmatize=False, stem=False)\n",
    "    lemmatized = preprocess_text(text, lemmatize=True, stem=False)\n",
    "    stemmed = preprocess_text(text, lemmatize=False, stem=True)\n",
    "    nouns = preprocess_text(text, lemmatize=True, pos_filter=['NN', 'NNS'])\n",
    "    \n",
    "    # Create a comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        'Original': [original],\n",
    "        'Tokenized': [' '.join(tokenized)],\n",
    "        'Lemmatized': [' '.join(lemmatized)],\n",
    "        'Stemmed': [' '.join(stemmed)],\n",
    "        'Nouns Only': [' '.join(nouns)]\n",
    "    })\n",
    "    \n",
    "    return comparison.T.rename(columns={0: 'Result'})\n",
    "\n",
    "# Display a detailed comparison for a short example\n",
    "sample_text = \"The customer complained about the steering wheel vibrating when braking. Replaced the front rotors and performed wheel alignment.\"\n",
    "comparison_df = compare_preprocessing(sample_text)\n",
    "print(\"\\nDetailed Preprocessing Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067220df",
   "metadata": {},
   "source": [
    "## 4. Part-of-Speech Tagging and Named Entity Recognition\n",
    "\n",
    "For generating meaningful tags from technical text like vehicle repair descriptions, identifying the right types of words is crucial. Two NLTK techniques are particularly useful for this:\n",
    "\n",
    "### Part-of-Speech (POS) Tagging\n",
    "\n",
    "**What is it?** \n",
    "POS tagging is the process of marking up each word in text with its corresponding part of speech (noun, verb, adjective, etc.).\n",
    "\n",
    "**Why is it useful for tag generation?**\n",
    "- **Nouns** are usually the most important for technical texts, as they identify parts, components, and systems\n",
    "- **Adjectives** often describe conditions or qualities of parts\n",
    "- **Verbs** indicate actions or issues\n",
    "- Filtering by specific POS tags helps focus on the most relevant terms\n",
    "\n",
    "**NLTK Implementation:**\n",
    "- `nltk.pos_tag()` assigns tags like NN (noun), VB (verb), JJ (adjective)\n",
    "- Common useful tags:\n",
    "  - NN, NNS: Singular and plural nouns\n",
    "  - JJ: Adjectives\n",
    "  - VB, VBG, VBN: Verbs in different forms\n",
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "**What is it?**\n",
    "NER identifies named entities in text and classifies them into predefined categories like person names, organizations, locations, etc.\n",
    "\n",
    "**Why is it useful for tag generation?**\n",
    "- Can identify specific parts or components that might be proper nouns\n",
    "- Helps extract structured information from unstructured text\n",
    "- Captures entities that might be missed by simple word frequency analysis\n",
    "\n",
    "**NLTK Implementation:**\n",
    "- `nltk.ne_chunk()` identifies named entities\n",
    "\n",
    "Let's implement these techniques on our vehicle repair data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49859ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging to the text data\n",
    "def pos_tag_text(text):\n",
    "    \"\"\"\n",
    "    Perform part-of-speech tagging on text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Tokenize and tag\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    return tagged\n",
    "\n",
    "# Extract words by POS tag\n",
    "def extract_by_pos(tagged_text, target_tags):\n",
    "    \"\"\"\n",
    "    Extract words that match specific POS tags\n",
    "    \n",
    "    Args:\n",
    "        tagged_text (list): List of (word, tag) tuples\n",
    "        target_tags (list): List of target POS tags\n",
    "        \n",
    "    Returns:\n",
    "        list: Words that match the target tags\n",
    "    \"\"\"\n",
    "    return [word for word, tag in tagged_text if tag in target_tags]\n",
    "\n",
    "# Apply POS tagging to a sample of documents\n",
    "sample_size = min(100, len(df))\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "\n",
    "pos_results = []\n",
    "for idx in sample_indices:\n",
    "    text = df['combined_text'].iloc[idx]\n",
    "    tagged = pos_tag_text(text)\n",
    "    \n",
    "    # Extract words by type\n",
    "    nouns = extract_by_pos(tagged, ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    verbs = extract_by_pos(tagged, ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    adjectives = extract_by_pos(tagged, ['JJ', 'JJR', 'JJS'])\n",
    "    \n",
    "    pos_results.append({\n",
    "        'index': idx,\n",
    "        'nouns': nouns,\n",
    "        'verbs': verbs,\n",
    "        'adjectives': adjectives\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "pos_df = pd.DataFrame(pos_results)\n",
    "\n",
    "# Display examples\n",
    "print(\"POS Tagging Examples:\")\n",
    "for i in range(min(3, len(pos_df))):\n",
    "    print(f\"\\nDocument {pos_df['index'].iloc[i]}:\")\n",
    "    print(f\"Nouns: {', '.join(pos_df['nouns'].iloc[i][:10])}\")\n",
    "    print(f\"Verbs: {', '.join(pos_df['verbs'].iloc[i][:10])}\")\n",
    "    print(f\"Adjectives: {', '.join(pos_df['adjectives'].iloc[i][:10])}\")\n",
    "\n",
    "# Visualize distribution of parts of speech\n",
    "pos_counts = {\n",
    "    'Nouns': np.mean([len(x) for x in pos_df['nouns']]),\n",
    "    'Verbs': np.mean([len(x) for x in pos_df['verbs']]),\n",
    "    'Adjectives': np.mean([len(x) for x in pos_df['adjectives']])\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pos_counts.keys(), pos_counts.values(), color=['blue', 'green', 'red'])\n",
    "plt.title('Average Number of Words by Part of Speech')\n",
    "plt.ylabel('Average Count per Document')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Named Entity Recognition\n",
    "print(\"\\nNamed Entity Recognition Examples:\")\n",
    "\n",
    "def perform_ner(text):\n",
    "    \"\"\"\n",
    "    Perform Named Entity Recognition on text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        Tree: NLTK Tree containing named entities\n",
    "    \"\"\"\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Tokenize, tag, and chunk\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    entities = ne_chunk(tagged)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply NER to a few example documents\n",
    "for i in range(3):\n",
    "    if i < len(df):\n",
    "        text = df['combined_text'].iloc[i]\n",
    "        entities = perform_ner(text)\n",
    "        \n",
    "        # Extract named entities\n",
    "        named_entities = []\n",
    "        for subtree in entities:\n",
    "            if isinstance(subtree, nltk.Tree):\n",
    "                entity_type = subtree.label()\n",
    "                entity_text = ' '.join([word for word, tag in subtree.leaves()])\n",
    "                named_entities.append((entity_text, entity_type))\n",
    "        \n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Text: {text[:150]}...\")\n",
    "        print(f\"Named Entities: {named_entities}\")\n",
    "\n",
    "# Create a function to generate tags based on POS and NER\n",
    "def generate_pos_tags(text, num_nouns=5, num_verbs=3, num_adj=2):\n",
    "    \"\"\"\n",
    "    Generate tags based on POS tagging\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        num_nouns (int): Number of noun tags to include\n",
    "        num_verbs (int): Number of verb tags to include\n",
    "        num_adj (int): Number of adjective tags to include\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated tags\n",
    "    \"\"\"\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Process text\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    \n",
    "    # Tag parts of speech\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Extract by part of speech\n",
    "    nouns = extract_by_pos(tagged, ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    verbs = extract_by_pos(tagged, ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    adjectives = extract_by_pos(tagged, ['JJ', 'JJR', 'JJS'])\n",
    "    \n",
    "    # Count frequencies\n",
    "    noun_counts = Counter(nouns)\n",
    "    verb_counts = Counter(verbs)\n",
    "    adj_counts = Counter(adjectives)\n",
    "    \n",
    "    # Get the most common words of each type\n",
    "    top_nouns = [word for word, _ in noun_counts.most_common(num_nouns)]\n",
    "    top_verbs = [word for word, _ in verb_counts.most_common(num_verbs)]\n",
    "    top_adjs = [word for word, _ in adj_counts.most_common(num_adj)]\n",
    "    \n",
    "    # Combine into tags\n",
    "    tags = top_nouns + top_verbs + top_adjs\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply tag generation to all documents\n",
    "df['pos_tags'] = df['combined_text'].apply(generate_pos_tags)\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nPOS-based Tag Generation Examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(df):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Text: {df['combined_text'].iloc[i][:100]}...\")\n",
    "        print(f\"POS Tags: {', '.join(df['pos_tags'].iloc[i])}\")\n",
    "\n",
    "# Compare with the original regex-based tags if they exist\n",
    "if 'TAGS' in df.columns:\n",
    "    print(\"\\nComparison with original regex-based tags:\")\n",
    "    for i in range(5):\n",
    "        if i < len(df):\n",
    "            print(f\"\\nDocument {i}:\")\n",
    "            print(f\"Original Tags: {df['TAGS'].iloc[i]}\")\n",
    "            print(f\"POS Tags: {', '.join(df['pos_tags'].iloc[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabfec5",
   "metadata": {},
   "source": [
    "## 5. Frequency Analysis and Collocation Detection\n",
    "\n",
    "Beyond individual words, understanding how words appear together can provide deeper insights. NLTK offers powerful tools for frequency analysis and collocation detection.\n",
    "\n",
    "### Frequency Analysis\n",
    "\n",
    "**What is it?**\n",
    "Counting how often words appear in a text or corpus.\n",
    "\n",
    "**Why is it useful for tag generation?**\n",
    "- Identifies the most common terms in repair descriptions\n",
    "- Highlights potentially important components or issues\n",
    "- Provides a baseline for understanding the repair data\n",
    "\n",
    "**NLTK Implementation:**\n",
    "- `nltk.FreqDist()` creates a frequency distribution of tokens\n",
    "\n",
    "### N-grams\n",
    "\n",
    "**What is it?**\n",
    "Contiguous sequences of n items (words) from a text.\n",
    "\n",
    "**Why is it useful for tag generation?**\n",
    "- Captures multi-word concepts (e.g., \"power steering\", \"front wheel\")\n",
    "- Preserves contextual information lost in single-word analysis\n",
    "- Often more specific and meaningful than single words\n",
    "\n",
    "**NLTK Implementation:**\n",
    "- `nltk.ngrams()` generates n-grams from text\n",
    "\n",
    "### Collocations\n",
    "\n",
    "**What is it?**\n",
    "Words that frequently appear together, beyond what would be expected by chance.\n",
    "\n",
    "**Why is it useful for tag generation?**\n",
    "- Identifies meaningful word combinations specific to the domain\n",
    "- Captures technical terms that consist of multiple words\n",
    "- More accurate than simple n-gram frequency counting\n",
    "\n",
    "**NLTK Implementation:**\n",
    "- `nltk.collocations.BigramCollocationFinder`\n",
    "- Uses statistical measures like PMI (Pointwise Mutual Information) to find significant word pairs\n",
    "\n",
    "Let's apply these techniques to our repair data to generate more meaningful tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8222186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus for frequency analysis\n",
    "all_words = []\n",
    "for tokens in df['lemmatized']:\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "# Create a frequency distribution\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# Plot the 20 most common words\n",
    "plt.figure(figsize=(12, 6))\n",
    "fdist.plot(20, title='20 Most Common Words in Repair Descriptions')\n",
    "plt.show()\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800, \n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    max_words=100\n",
    ").generate(' '.join(all_words))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Repair Description Terms')\n",
    "plt.show()\n",
    "\n",
    "# Generate n-grams (bigrams and trigrams)\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "        n (int): Size of n-grams\n",
    "        \n",
    "    Returns:\n",
    "        list: List of n-grams\n",
    "    \"\"\"\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Combine all preprocessed tokens for n-gram analysis\n",
    "all_tokens = []\n",
    "for tokens in df['lemmatized']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Generate bigrams and trigrams\n",
    "bigrams_list = generate_ngrams(all_tokens, 2)\n",
    "trigrams_list = generate_ngrams(all_tokens, 3)\n",
    "\n",
    "# Count frequencies\n",
    "bigram_freq = Counter(bigrams_list)\n",
    "trigram_freq = Counter(trigrams_list)\n",
    "\n",
    "# Display the most common n-grams\n",
    "print(\"Most Common Bigrams (word pairs):\")\n",
    "for gram, count in bigram_freq.most_common(15):\n",
    "    print(f\"{' '.join(gram)}: {count}\")\n",
    "\n",
    "print(\"\\nMost Common Trigrams (three-word sequences):\")\n",
    "for gram, count in trigram_freq.most_common(15):\n",
    "    print(f\"{' '.join(gram)}: {count}\")\n",
    "\n",
    "# Find collocations (statistically significant bigrams)\n",
    "# First, prepare the text for collocation finding\n",
    "all_text_for_collocations = []\n",
    "for text in df['combined_text']:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    all_text_for_collocations.extend(tokens)\n",
    "\n",
    "# Create a collocation finder\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_text_for_collocations)\n",
    "\n",
    "# Apply frequency filter\n",
    "bigram_finder.apply_freq_filter(3)  # Minimum frequency of 3\n",
    "\n",
    "# Find top collocations using PMI (Pointwise Mutual Information)\n",
    "top_collocations = bigram_finder.nbest(BigramAssocMeasures.pmi, 20)\n",
    "\n",
    "print(\"\\nTop Collocations (Statistically Significant Word Pairs):\")\n",
    "for collocation in top_collocations:\n",
    "    print(' '.join(collocation))\n",
    "\n",
    "# Generate tags based on frequency, n-grams, and collocations\n",
    "def generate_ngram_tags(text, unigram_count=3, bigram_count=3, top_collocations=top_collocations):\n",
    "    \"\"\"\n",
    "    Generate tags based on word frequency and collocations\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        unigram_count (int): Number of single-word tags to include\n",
    "        bigram_count (int): Number of two-word tags to include\n",
    "        top_collocations (list): List of top collocations to match against\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated tags\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    tokens = preprocess_text(text, tokenize=True, remove_stopwords=True, lemmatize=True)\n",
    "    \n",
    "    # Generate tags from single words (unigrams)\n",
    "    word_counts = Counter(tokens)\n",
    "    unigram_tags = [word for word, _ in word_counts.most_common(unigram_count)]\n",
    "    \n",
    "    # Generate bigrams and check for matches with top collocations\n",
    "    text_bigrams = list(ngrams(tokens, 2))\n",
    "    bigram_tags = []\n",
    "    \n",
    "    # First check for collocations in the text\n",
    "    for bigram in text_bigrams:\n",
    "        if bigram in top_collocations:\n",
    "            bigram_tags.append(' '.join(bigram))\n",
    "    \n",
    "    # If we don't have enough collocation matches, add frequent bigrams\n",
    "    if len(bigram_tags) < bigram_count:\n",
    "        bigram_counts = Counter(text_bigrams)\n",
    "        for bigram, _ in bigram_counts.most_common(bigram_count - len(bigram_tags)):\n",
    "            bigram_tags.append(' '.join(bigram))\n",
    "    \n",
    "    # Combine tags (limit to requested counts)\n",
    "    tags = unigram_tags[:unigram_count] + bigram_tags[:bigram_count]\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply n-gram tag generation to all documents\n",
    "df['ngram_tags'] = df['combined_text'].apply(generate_ngram_tags)\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nN-gram Based Tag Generation Examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(df):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Text: {df['combined_text'].iloc[i][:100]}...\")\n",
    "        print(f\"N-gram Tags: {', '.join(df['ngram_tags'].iloc[i])}\")\n",
    "\n",
    "# Compare with POS tags\n",
    "print(\"\\nComparison with POS-based tags:\")\n",
    "for i in range(5):\n",
    "    if i < len(df):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"POS Tags: {', '.join(df['pos_tags'].iloc[i])}\")\n",
    "        print(f\"N-gram Tags: {', '.join(df['ngram_tags'].iloc[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e66a76",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Tag Generation Using NLTK\n",
    "\n",
    "Now that we've explored individual NLTK techniques, let's combine them into a comprehensive tag generation approach. This will leverage:\n",
    "\n",
    "1. **Preprocessing** to clean and normalize the text\n",
    "2. **POS Tagging** to identify nouns, verbs, and adjectives\n",
    "3. **N-grams** to capture multi-word concepts\n",
    "4. **Collocations** to find statistically significant word pairs\n",
    "5. **Frequency Analysis** to prioritize common terms\n",
    "6. **Named Entity Recognition** to identify specific components\n",
    "\n",
    "The goal is to generate more meaningful and contextually relevant tags than the basic regex approach. Our comprehensive approach will:\n",
    "\n",
    "- Prioritize technical nouns (parts, components)\n",
    "- Include important descriptive adjectives (conditions, symptoms)\n",
    "- Add action verbs (problem indicators)\n",
    "- Incorporate significant word combinations\n",
    "- Filter out irrelevant or common terms\n",
    "\n",
    "Let's implement this comprehensive approach and evaluate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified yet effective tag generation function\n",
    "def generate_simple_nltk_tags(text, max_tags=6):\n",
    "    \"\"\"\n",
    "    Generate tags using NLTK with a simplified approach\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        max_tags (int): Maximum number of tags to generate\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated tags\n",
    "    \"\"\"\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Convert to lowercase and clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Get part-of-speech tags\n",
    "    tagged = nltk.pos_tag(lemmatized)\n",
    "    \n",
    "    # Extract nouns (most important for technical text)\n",
    "    nouns = [word for word, tag in tagged if tag.startswith('N')]\n",
    "    \n",
    "    # Extract verbs and adjectives\n",
    "    verbs = [word for word, tag in tagged if tag.startswith('V')]\n",
    "    adjectives = [word for word, tag in tagged if tag.startswith('J')]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    noun_counts = Counter(nouns)\n",
    "    verb_counts = Counter(verbs)\n",
    "    adj_counts = Counter(adjectives)\n",
    "    \n",
    "    # Create list of bigrams for the most important multi-word terms\n",
    "    bigrams = list(nltk.bigrams(lemmatized))\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    # Generate tags in order of priority\n",
    "    tags = []\n",
    "    \n",
    "    # Add top nouns (most important for parts identification)\n",
    "    tags.extend([word for word, _ in noun_counts.most_common(3)])\n",
    "    \n",
    "    # Add top bigrams that appear more than once\n",
    "    for (w1, w2), count in bigram_counts.most_common(2):\n",
    "        if count > 1:\n",
    "            tags.append(f\"{w1} {w2}\")\n",
    "    \n",
    "    # Add top adjectives (for condition description)\n",
    "    tags.extend([word for word, _ in adj_counts.most_common(1)])\n",
    "    \n",
    "    # Add top verbs (for action/issue identification)\n",
    "    tags.extend([word for word, _ in verb_counts.most_common(1)])\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_tags = []\n",
    "    for tag in tags:\n",
    "        if tag not in unique_tags:\n",
    "            unique_tags.append(tag)\n",
    "    \n",
    "    # Limit to maximum number of tags\n",
    "    return unique_tags[:max_tags]\n",
    "\n",
    "# Apply the simplified tag generation to the data\n",
    "df['simple_nltk_tags'] = df['combined_text'].apply(generate_simple_nltk_tags)\n",
    "\n",
    "# Display examples\n",
    "print(\"Simple NLTK Tag Generation Examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(df):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Text: {df['combined_text'].iloc[i][:100]}...\")\n",
    "        print(f\"Simple NLTK Tags: {', '.join(df['simple_nltk_tags'].iloc[i])}\")\n",
    "\n",
    "# Compare with original regex tags if they exist\n",
    "if 'TAGS' in df.columns:\n",
    "    print(\"\\nComparison with original regex-based tags:\")\n",
    "    for i in range(5):\n",
    "        if i < len(df):\n",
    "            print(f\"\\nDocument {i}:\")\n",
    "            print(f\"Original Tags: {df['TAGS'].iloc[i]}\")\n",
    "            print(f\"Simple NLTK Tags: {', '.join(df['simple_nltk_tags'].iloc[i])}\")\n",
    "\n",
    "# Create a simple visualization to compare tag coverage\n",
    "# Create a list of common vehicle repair terms\n",
    "repair_terms = {\n",
    "    'wheel', 'steering', 'brake', 'engine', 'transmission', 'clutch',\n",
    "    'battery', 'suspension', 'exhaust', 'filter', 'sensor', 'valve', \n",
    "    'electrical', 'vibration', 'noise', 'leak', 'alignment',\n",
    "    'power', 'door', 'window', 'motor'\n",
    "}\n",
    "\n",
    "# Check how many of these terms appear in tags\n",
    "def count_repair_terms(tags, term_list):\n",
    "    \"\"\"Count how many repair terms are in the tags\"\"\"\n",
    "    if isinstance(tags, str):\n",
    "        # For original tags (comma-separated string)\n",
    "        tags = [t.strip() for t in tags.split(',')]\n",
    "    \n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        for term in term_list:\n",
    "            if term in tag.lower():\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "# Evaluate each method\n",
    "original_count = 0\n",
    "nltk_count = 0\n",
    "\n",
    "if 'TAGS' in df.columns:\n",
    "    original_count = sum(count_repair_terms(tags, repair_terms) \n",
    "                         for tags in df['TAGS'] if pd.notna(tags))\n",
    "\n",
    "nltk_count = sum(count_repair_terms(tags, repair_terms) \n",
    "                for tags in df['simple_nltk_tags'])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRepair Term Coverage:\")\n",
    "if 'TAGS' in df.columns:\n",
    "    print(f\"Original Regex Tags: {original_count} repair terms identified\")\n",
    "print(f\"Simple NLTK Tags: {nltk_count} repair terms identified\")\n",
    "\n",
    "# Visualize the comparison if both methods are available\n",
    "if 'TAGS' in df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(['Original Regex', 'Simple NLTK'], [original_count, nltk_count], \n",
    "            color=['blue', 'green'])\n",
    "    plt.title('Repair Term Coverage by Tag Generation Method')\n",
    "    plt.ylabel('Number of Repair Terms Identified')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Save the tagged data\n",
    "df.to_csv('vehicle_repair_simple_nltk_tags.csv', index=False)\n",
    "print(\"\\nSaved tagged data to 'vehicle_repair_simple_nltk_tags.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2fd58",
   "metadata": {},
   "source": [
    "## 7. Why NLTK is Better Than Regex for Tag Generation (Interview Explanation)\n",
    "\n",
    "If you're asked in an interview why you chose NLTK over simple regex for tag generation, here's a concise explanation:\n",
    "\n",
    "### 1. Language Understanding vs. Pattern Matching\n",
    "\n",
    "**Regex:** Simply matches character patterns without any understanding of language.\n",
    "**NLTK:** Understands language components (nouns, verbs, etc.) and their relationships.\n",
    "\n",
    "### 2. Context Awareness\n",
    "\n",
    "**Regex:** Treats each word independently, missing context and relationships.\n",
    "**NLTK:** Can recognize multi-word phrases and how words relate to each other.\n",
    "\n",
    "### 3. Built-in Language Resources\n",
    "\n",
    "**Regex:** No built-in knowledge of language. You have to manually create rules for everything.\n",
    "**NLTK:** Comes with pre-built resources like stopword lists, lemmatizers, and POS taggers.\n",
    "\n",
    "### 4. Intelligent Processing\n",
    "\n",
    "**Regex:** Can only do what you explicitly program it to do.\n",
    "**NLTK:** Can perform intelligent operations like finding root words, identifying parts of speech, and recognizing named entities.\n",
    "\n",
    "### 5. Practical Example\n",
    "\n",
    "**Regex Approach:** Might identify \"running\" and \"ran\" as completely different words.\n",
    "**NLTK Approach:** Can recognize both \"running\" and \"ran\" derive from \"run\" using lemmatization.\n",
    "\n",
    "### 6. Real-world Benefit in Vehicle Repair Data\n",
    "\n",
    "**Regex Limitation:** Might just identify \"steering\" and \"wheel\" as separate words.\n",
    "**NLTK Advantage:** Can recognize \"steering wheel\" as a single important component and prioritize technical nouns over common words.\n",
    "\n",
    "The simple NLTK approach we've implemented maintains good accuracy while being much easier to explain and understand than complex machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example to demonstrate NLTK vs. regex for an interview\n",
    "sample_text = \"Customer complained about steering wheel vibration when braking. Replaced front rotors and performed wheel alignment.\"\n",
    "\n",
    "print(\"Interview Example: NLTK vs. Regex\")\n",
    "print(\"Sample text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n1. Simple Regex Approach (from previous notebook):\")\n",
    "\n",
    "# Simulate regex approach (similar to original notebook)\n",
    "def regex_tag_generation(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split into words\n",
    "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    # Remove common words (simplified stopword list)\n",
    "    stop_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'with', 'about', 'when']\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words)\n",
    "    # Get top words as tags\n",
    "    tags = [word for word, _ in word_counts.most_common(5)]\n",
    "    return tags\n",
    "\n",
    "regex_tags = regex_tag_generation(sample_text)\n",
    "print(f\"Regex Tags: {', '.join(regex_tags)}\")\n",
    "\n",
    "print(\"\\n2. NLTK Approach:\")\n",
    "nltk_tags = generate_simple_nltk_tags(sample_text)\n",
    "print(f\"NLTK Tags: {', '.join(nltk_tags)}\")\n",
    "\n",
    "print(\"\\nKey Differences to Highlight in an Interview:\")\n",
    "print(\"1. NLTK identifies 'steering wheel' as a single component\")\n",
    "print(\"2. NLTK prioritizes nouns (parts) like 'rotor' over common words\")\n",
    "print(\"3. NLTK understands words like 'replaced' and 'performed' are verbs (actions)\")\n",
    "print(\"4. NLTK can normalize different word forms (e.g., 'braking' → 'brake')\")\n",
    "print(\"5. NLTK requires minimal manual configuration compared to regex\")\n",
    "\n",
    "print(\"\\nSimple Technical Implementation:\")\n",
    "print(\"1. Import NLTK: 'import nltk'\")\n",
    "print(\"2. Preprocess text: lowercase, clean, tokenize\")\n",
    "print(\"3. Remove stopwords using NLTK's built-in list\")\n",
    "print(\"4. Apply lemmatization to get base word forms\")\n",
    "print(\"5. Use POS tagging to identify nouns, verbs, etc.\")\n",
    "print(\"6. Generate tags based on word type and frequency\")\n",
    "print(\"7. Include important bigrams for multi-word concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60fed6",
   "metadata": {},
   "source": [
    "## 5. Text Classification with scikit-learn\n",
    "\n",
    "Now we'll use a supervised learning approach where we train a classifier to predict relevant tags based on text features. This is a more straightforward approach than deep learning but can still be very effective.\n",
    "\n",
    "For this example, we'll:\n",
    "1. Use our previously generated TF-IDF tags as \"ground truth\"\n",
    "2. Train a simple logistic regression classifier to predict these tags\n",
    "3. Evaluate the model's performance\n",
    "4. Generate tags for new documents using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1527c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we'll use our TF-IDF tags as ground truth\n",
    "# In a real scenario, you would use manually labeled data\n",
    "if 'tfidf_tags' in df.columns and len(df['tfidf_tags'].iloc[0]) > 0:\n",
    "    # We'll create a multi-label classification problem\n",
    "    # First, get all unique tags\n",
    "    all_tags = set()\n",
    "    for tags in df['tfidf_tags']:\n",
    "        all_tags.update(tags)\n",
    "    \n",
    "    print(f\"Total unique tags: {len(all_tags)}\")\n",
    "    \n",
    "    # Convert the tags to a multi-hot encoded format\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    tag_matrix = mlb.fit_transform(df['tfidf_tags'])\n",
    "    \n",
    "    print(f\"Tag matrix shape: {tag_matrix.shape}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        tfidf_matrix, tag_matrix, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    \n",
    "    # We'll use a LogisticRegression classifier for each tag\n",
    "    print(\"\\nTraining multi-label classifier...\")\n",
    "    \n",
    "    # Initialize the multi-output classifier with logistic regression\n",
    "    clf = MultiOutputClassifier(LogisticRegression(max_iter=1000, C=1.0))\n",
    "    \n",
    "    # Train the classifier\n",
    "    try:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy for each tag\n",
    "        tag_accuracies = []\n",
    "        for i in range(tag_matrix.shape[1]):\n",
    "            acc = accuracy_score(y_test[:, i], y_pred[:, i])\n",
    "            tag_accuracies.append(acc)\n",
    "        \n",
    "        print(f\"Average tag prediction accuracy: {np.mean(tag_accuracies):.4f}\")\n",
    "        \n",
    "        # Generate classifier-based tags for all documents\n",
    "        predicted_tags = clf.predict(tfidf_matrix)\n",
    "        \n",
    "        # Convert back to lists of tag names\n",
    "        classifier_tags = []\n",
    "        for i in range(len(predicted_tags)):\n",
    "            tags = mlb.inverse_transform(predicted_tags[i:i+1])[0]\n",
    "            classifier_tags.append(list(tags))\n",
    "        \n",
    "        # Add to dataframe\n",
    "        df['classifier_tags'] = classifier_tags\n",
    "        \n",
    "        # Display examples\n",
    "        print(\"\\nExamples of classifier-based tags:\")\n",
    "        for i in range(5):\n",
    "            print(f\"\\nDocument {i+1}:\")\n",
    "            print(f\"Text: {df['combined_text'].iloc[i][:100]}...\")\n",
    "            print(f\"Classifier Tags: {', '.join(df['classifier_tags'].iloc[i])}\")\n",
    "            print(f\"TF-IDF Tags (Ground Truth): {', '.join(df['tfidf_tags'].iloc[i])}\")\n",
    "            \n",
    "        # Feature importance for tag prediction\n",
    "        # Get a sample tag and its corresponding classifier\n",
    "        if len(all_tags) > 0:\n",
    "            sample_tag_idx = 0\n",
    "            sample_tag = list(all_tags)[sample_tag_idx]\n",
    "            sample_clf = clf.estimators_[sample_tag_idx]\n",
    "            \n",
    "            # Get feature importance for this tag\n",
    "            if hasattr(sample_clf, 'coef_'):\n",
    "                # Get the feature names\n",
    "                feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "                \n",
    "                # Get the coefficient values\n",
    "                coefficients = sample_clf.coef_[0]\n",
    "                \n",
    "                # Create a dataframe with feature names and coefficients\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Importance': coefficients\n",
    "                })\n",
    "                \n",
    "                # Sort by absolute importance\n",
    "                feature_importance['Abs_Importance'] = np.abs(feature_importance['Importance'])\n",
    "                feature_importance = feature_importance.sort_values('Abs_Importance', ascending=False)\n",
    "                \n",
    "                # Display the top 10 most important features\n",
    "                print(f\"\\nTop 10 most important features for predicting tag '{sample_tag}':\")\n",
    "                print(feature_importance.head(10))\n",
    "                \n",
    "                # Visualize feature importance\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                top_features = feature_importance.head(15)\n",
    "                colors = ['green' if x > 0 else 'red' for x in top_features['Importance']]\n",
    "                plt.barh(top_features['Feature'], top_features['Importance'], color=colors)\n",
    "                plt.title(f'Feature Importance for Tag: {sample_tag}')\n",
    "                plt.xlabel('Coefficient Value')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training classifier: {e}\")\n",
    "        print(\"Skipping classifier-based tag generation\")\n",
    "        df['classifier_tags'] = [[] for _ in range(len(df))]\n",
    "else:\n",
    "    print(\"TF-IDF tags not available or empty. Skipping classifier-based tag generation.\")\n",
    "    df['classifier_tags'] = [[] for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8647d3",
   "metadata": {},
   "source": [
    "## 6. Comparing Tag Generation Methods\n",
    "\n",
    "Now let's compare the effectiveness of the different tag generation methods:\n",
    "1. Original regex-based approach\n",
    "2. TF-IDF based approach\n",
    "3. LDA topic modeling approach\n",
    "4. K-means clustering approach\n",
    "5. Classifier-based approach\n",
    "\n",
    "We'll evaluate the quality of tags and compare the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d462dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tag columns to compare\n",
    "tag_columns = []\n",
    "if 'TAGS' in df.columns:\n",
    "    tag_columns.append('TAGS')\n",
    "if 'tfidf_tags' in df.columns:\n",
    "    tag_columns.append('tfidf_tags')\n",
    "if 'lda_tags' in df.columns:\n",
    "    tag_columns.append('lda_tags')\n",
    "if 'cluster_tags' in df.columns:\n",
    "    tag_columns.append('cluster_tags')\n",
    "if 'classifier_tags' in df.columns:\n",
    "    tag_columns.append('classifier_tags')\n",
    "\n",
    "# Function to calculate tag metrics\n",
    "def calculate_tag_metrics(df, tag_columns):\n",
    "    \"\"\"\n",
    "    Calculate metrics for tag columns\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with tag columns\n",
    "        tag_columns: List of column names containing tags\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics for each tag column\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for col in tag_columns:\n",
    "        # Count number of tags per document\n",
    "        if col == 'TAGS':\n",
    "            # Original tags might be comma-separated strings\n",
    "            tag_counts = df[col].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0)\n",
    "        else:\n",
    "            # Other tag columns are lists\n",
    "            tag_counts = df[col].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics[col] = {\n",
    "            'avg_tags_per_doc': tag_counts.mean(),\n",
    "            'median_tags_per_doc': tag_counts.median(),\n",
    "            'max_tags_per_doc': tag_counts.max(),\n",
    "            'min_tags_per_doc': tag_counts.min(),\n",
    "            'docs_with_tags': (tag_counts > 0).sum(),\n",
    "            'docs_without_tags': (tag_counts == 0).sum()\n",
    "        }\n",
    "        \n",
    "        # Get unique tags\n",
    "        if col == 'TAGS':\n",
    "            # Original tags might be comma-separated strings\n",
    "            all_tags = set()\n",
    "            for tags in df[col]:\n",
    "                if pd.notna(tags):\n",
    "                    all_tags.update([t.strip() for t in str(tags).split(',')])\n",
    "        else:\n",
    "            # Other tag columns are lists\n",
    "            all_tags = set()\n",
    "            for tags in df[col]:\n",
    "                if isinstance(tags, list):\n",
    "                    all_tags.update(tags)\n",
    "        \n",
    "        metrics[col]['unique_tags'] = len(all_tags)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "tag_metrics = calculate_tag_metrics(df, tag_columns)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Tag Generation Method Comparison:\\n\")\n",
    "print(\"{:<20} {:<15} {:<15} {:<15} {:<15} {:<15}\".format(\n",
    "    \"Method\", \"Avg Tags/Doc\", \"Median Tags/Doc\", \"Docs With Tags\", \"Docs Without Tags\", \"Unique Tags\"\n",
    "))\n",
    "print(\"-\" * 95)\n",
    "for col, m in tag_metrics.items():\n",
    "    print(\"{:<20} {:<15.2f} {:<15.1f} {:<15} {:<15} {:<15}\".format(\n",
    "        col,\n",
    "        m['avg_tags_per_doc'],\n",
    "        m['median_tags_per_doc'],\n",
    "        m['docs_with_tags'],\n",
    "        m['docs_without_tags'],\n",
    "        m['unique_tags']\n",
    "    ))\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Average tags per document\n",
    "plt.subplot(1, 2, 1)\n",
    "avg_tags = [m['avg_tags_per_doc'] for m in tag_metrics.values()]\n",
    "plt.bar(tag_metrics.keys(), avg_tags)\n",
    "plt.title('Average Tags per Document')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Number of Tags')\n",
    "\n",
    "# Unique tags\n",
    "plt.subplot(1, 2, 2)\n",
    "unique_tags = [m['unique_tags'] for m in tag_metrics.values()]\n",
    "plt.bar(tag_metrics.keys(), unique_tags)\n",
    "plt.title('Number of Unique Tags')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display examples comparing all methods\n",
    "print(\"\\nTag Comparison Examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Text: {df['combined_text'].iloc[i][:150]}...\")\n",
    "    for col in tag_columns:\n",
    "        if col == 'TAGS':\n",
    "            tags = df[col].iloc[i] if pd.notna(df[col].iloc[i]) else \"\"\n",
    "            print(f\"{col}: {tags}\")\n",
    "        else:\n",
    "            tags = ', '.join(df[col].iloc[i]) if isinstance(df[col].iloc[i], list) else \"\"\n",
    "            print(f\"{col}: {tags}\")\n",
    "\n",
    "# Create a function to calculate tag overlap between methods\n",
    "def calculate_tag_overlap(df, method1, method2):\n",
    "    \"\"\"\n",
    "    Calculate the overlap between tags from two different methods\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with tag columns\n",
    "        method1: First tag method column name\n",
    "        method2: Second tag method column name\n",
    "        \n",
    "    Returns:\n",
    "        float: Average Jaccard similarity between tag sets\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        # Get tags from method 1\n",
    "        if method1 == 'TAGS':\n",
    "            tags1 = set(str(df[method1].iloc[i]).split(',')) if pd.notna(df[method1].iloc[i]) else set()\n",
    "        else:\n",
    "            tags1 = set(df[method1].iloc[i]) if isinstance(df[method1].iloc[i], list) else set()\n",
    "        \n",
    "        # Get tags from method 2\n",
    "        if method2 == 'TAGS':\n",
    "            tags2 = set(str(df[method2].iloc[i]).split(',')) if pd.notna(df[method2].iloc[i]) else set()\n",
    "        else:\n",
    "            tags2 = set(df[method2].iloc[i]) if isinstance(df[method2].iloc[i], list) else set()\n",
    "        \n",
    "        # Calculate Jaccard similarity (intersection over union)\n",
    "        if tags1 or tags2:  # Avoid division by zero\n",
    "            similarity = len(tags1.intersection(tags2)) / len(tags1.union(tags2))\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    return np.mean(similarities) if similarities else 0\n",
    "\n",
    "# Calculate overlap between methods\n",
    "if len(tag_columns) > 1:\n",
    "    print(\"\\nTag Overlap Between Methods (Jaccard Similarity):\")\n",
    "    overlap_matrix = np.zeros((len(tag_columns), len(tag_columns)))\n",
    "    \n",
    "    for i, method1 in enumerate(tag_columns):\n",
    "        for j, method2 in enumerate(tag_columns):\n",
    "            if i != j:\n",
    "                overlap = calculate_tag_overlap(df, method1, method2)\n",
    "                overlap_matrix[i, j] = overlap\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    overlap_df = pd.DataFrame(overlap_matrix, index=tag_columns, columns=tag_columns)\n",
    "    \n",
    "    # Display the overlap matrix\n",
    "    print(overlap_df)\n",
    "    \n",
    "    # Visualize the overlap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(overlap_df, annot=True, cmap='YlGnBu', vmin=0, vmax=1)\n",
    "    plt.title('Tag Overlap Between Methods (Jaccard Similarity)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d8587",
   "metadata": {},
   "source": [
    "## 7. Conclusion: Advantages of scikit-learn Approaches\n",
    "\n",
    "Based on our exploration of different tag generation methods, we can draw some conclusions about the advantages of scikit-learn approaches over basic regex:\n",
    "\n",
    "1. **Statistical Significance**: TF-IDF automatically weights terms based on their importance in the document and corpus.\n",
    "\n",
    "2. **Topic Discovery**: LDA identifies hidden topics and themes that might not be apparent through simple word counting.\n",
    "\n",
    "3. **Document Clustering**: K-means groups similar documents together, helping to identify common issues or repair types.\n",
    "\n",
    "4. **Supervised Learning**: Classifier-based approaches can learn patterns from existing tags to apply to new documents.\n",
    "\n",
    "5. **Reduced Manual Effort**: These approaches reduce the need for manually crafting rules or stopword lists.\n",
    "\n",
    "Let's save our enriched dataset with the tags from all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with all tag columns\n",
    "output_file = 'vehicle_repair_sklearn_tags.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved enriched dataset with all tag methods to '{output_file}'\")\n",
    "\n",
    "# Provide a summary of the advantages of each method\n",
    "tag_methods = {\n",
    "    'Regex (Original)': [\n",
    "        'Simple to implement and understand',\n",
    "        'Fast computation',\n",
    "        'No training required',\n",
    "        'Works with limited data'\n",
    "    ],\n",
    "    'TF-IDF': [\n",
    "        'Prioritizes distinctive words',\n",
    "        'Reduces importance of common words automatically',\n",
    "        'Captures document-specific terminology',\n",
    "        'Simple yet effective for document characterization'\n",
    "    ],\n",
    "    'LDA Topic Modeling': [\n",
    "        'Discovers latent topics across the corpus',\n",
    "        'Groups related terms together',\n",
    "        'Provides insights into document themes',\n",
    "        'Works well for category/taxonomy generation'\n",
    "    ],\n",
    "    'K-means Clustering': [\n",
    "        'Groups similar documents together',\n",
    "        'Identifies common repair issues or types',\n",
    "        'Provides consistent tags within clusters',\n",
    "        'Easy to interpret and visualize'\n",
    "    ],\n",
    "    'Classifier-based': [\n",
    "        'Can be trained on expert-labeled tags',\n",
    "        'Adapts to domain-specific tagging patterns',\n",
    "        'Consistent tag application across documents',\n",
    "        'Good for standardized tagging systems'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\nAdvantages of Different Tag Generation Methods:\")\n",
    "for method, advantages in tag_methods.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    for adv in advantages:\n",
    "        print(f\"  • {adv}\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"For your vehicle repair dataset, a combination approach may work best:\")\n",
    "print(\"1. Use TF-IDF for initial tag candidates (good baseline)\")\n",
    "print(\"2. Apply LDA to identify major repair themes/categories\")\n",
    "print(\"3. Use K-means to group similar repair descriptions\")\n",
    "print(\"4. If you have labeled data, train a custom classifier for specific tag prediction\")\n",
    "print(\"\\nEach method has strengths and weaknesses - the best approach depends on your specific needs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372f22a",
   "metadata": {},
   "source": [
    "## 8. Practical Implementation Guide\n",
    "\n",
    "To implement these tag generation approaches in your production environment:\n",
    "\n",
    "1. **Start Simple**: Begin with TF-IDF based tagging as it provides a good balance of effectiveness and simplicity.\n",
    "\n",
    "2. **Evaluate Needs**: If you need to discover topics or themes, add LDA. If you need to group similar documents, add K-means clustering.\n",
    "\n",
    "3. **Add Supervision**: Once you have a good set of tagged documents, train a classifier for more consistent tagging.\n",
    "\n",
    "4. **Iterative Improvement**: Refine your tag generation system based on feedback and results.\n",
    "\n",
    "5. **Visualization**: Use the visualization techniques shown in this notebook to communicate findings to stakeholders.\n",
    "\n",
    "This moderate approach with scikit-learn gives you powerful text analysis capabilities without the complexity of deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization comparing the different tag generation methods\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: Tag count comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "method_names = list(tag_metrics.keys())\n",
    "avg_tags = [m['avg_tags_per_doc'] for m in tag_metrics.values()]\n",
    "plt.bar(method_names, avg_tags, color='skyblue')\n",
    "plt.title('Average Tags per Document')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Subplot 2: Unique tags comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "unique_tags = [m['unique_tags'] for m in tag_metrics.values()]\n",
    "plt.bar(method_names, unique_tags, color='lightgreen')\n",
    "plt.title('Unique Tags Generated')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Subplot 3: Docs with tags\n",
    "plt.subplot(2, 2, 3)\n",
    "docs_with_tags = [m['docs_with_tags'] / (m['docs_with_tags'] + m['docs_without_tags']) * 100 \n",
    "                  for m in tag_metrics.values()]\n",
    "plt.bar(method_names, docs_with_tags, color='salmon')\n",
    "plt.title('Documents with Tags (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Percentage')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Subplot 4: Method complexity vs. performance\n",
    "plt.subplot(2, 2, 4)\n",
    "# Estimated complexity (subjective rating)\n",
    "complexity = {\n",
    "    'TAGS': 1,               # Regex approach (simplest)\n",
    "    'tfidf_tags': 2,         # TF-IDF (simple)\n",
    "    'lda_tags': 3,           # LDA (moderate)\n",
    "    'cluster_tags': 3,       # K-means (moderate)\n",
    "    'classifier_tags': 4     # Classifier (more complex)\n",
    "}\n",
    "\n",
    "# Plot complexity vs. unique tags (as a proxy for performance)\n",
    "complexity_values = [complexity.get(method, 0) for method in method_names]\n",
    "plt.scatter(complexity_values, unique_tags, s=100, c='purple', alpha=0.7)\n",
    "\n",
    "# Add method labels\n",
    "for i, method in enumerate(method_names):\n",
    "    plt.annotate(method, (complexity_values[i], unique_tags[i]), \n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Method Complexity vs. Performance')\n",
    "plt.xlabel('Complexity (1=Simple, 4=Complex)')\n",
    "plt.ylabel('Unique Tags (Performance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tag_methods_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary visualization saved as 'tag_methods_comparison.png'\")\n",
    "print(\"\\nConclusion: The scikit-learn based approaches provide a significant improvement over\")\n",
    "print(\"basic regex tagging while remaining accessible and interpretable. These methods\")\n",
    "print(\"strike a good balance between complexity and performance for practical applications.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
